# Table of Contents for *Elements of Information Theory, Second Edition* by Thomas M. Cover and Joy A. Thomas

- [Preface to the Second Edition](#preface-to-the-second-edition) (xv)
- [Preface to the First Edition](#preface-to-the-first-edition) (xvii)
- [Acknowledgments for the Second Edition](#acknowledgments-for-the-second-edition) (xxi)
- [Acknowledgments for the First Edition](#acknowledgments-for-the-first-edition) (xxiii)

## [1. Introduction and Preview](#introduction-and-preview) (1)
- [1.1 Preview of the Book](#preview-of-the-book) (5)

## [2. Entropy, Relative Entropy, and Mutual Information](#entropy-relative-entropy-and-mutual-information) (13)
- [2.1 Entropy](#entropy) (13)
- [2.2 Joint Entropy and Conditional Entropy](#joint-entropy-and-conditional-entropy) (16)
- [2.3 Relative Entropy and Mutual Information](#relative-entropy-and-mutual-information) (19)
- [2.4 Relationship Between Entropy and Mutual Information](#relationship-between-entropy-and-mutual-information) (20)
- [2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information](#chain-rules-for-entropy-relative-entropy-and-mutual-information) (22)
- [2.6 Jensen’s Inequality and Its Consequences](#jensens-inequality-and-its-consequences) (25)
- [2.7 Log Sum Inequality and Its Applications](#log-sum-inequality-and-its-applications) (30)
- [2.8 Data-Processing Inequality](#data-processing-inequality) (34)
- [2.9 Sufficient Statistics](#sufficient-statistics) (35)
- [2.10 Fano’s Inequality](#fanos-inequality) (37)
- [Summary](#summary-2) (41)
- [Problems](#problems-2) (43)
- [Historical Notes](#historical-notes-2) (54)

## [3. Asymptotic Equipartition Property](#asymptotic-equipartition-property) (57)
- [3.1 Asymptotic Equipartition Property Theorem](#asymptotic-equipartition-property-theorem) (58)
- [3.2 Consequences of the AEP: Data Compression](#consequences-of-the-aep-data-compression) (60)
- [3.3 High-Probability Sets and the Typical Set](#high-probability-sets-and-the-typical-set) (62)
- [Summary](#summary-3) (64)
- [Problems](#problems-3) (64)
- [Historical Notes](#historical-notes-3) (69)

## [4. Entropy Rates of a Stochastic Process](#entropy-rates-of-a-stochastic-process) (71)
- [4.1 Markov Chains](#markov-chains) (71)
- [4.2 Entropy Rate](#entropy-rate) (74)
- [4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph](#example-entropy-rate-of-a-random-walk-on-a-weighted-graph) (78)
- [4.4 Second Law of Thermodynamics](#second-law-of-thermodynamics) (81)
- [4.5 Functions of Markov Chains](#functions-of-markov-chains) (84)
- [Summary](#summary-4) (87)
- [Problems](#problems-4) (88)
- [Historical Notes](#historical-notes-4) (100)

## [5. Data Compression](#data-compression) (103)
- [5.1 Examples of Codes](#examples-of-codes) (103)
- [5.2 Kraft Inequality](#kraft-inequality) (107)
- [5.3 Optimal Codes](#optimal-codes) (110)
- [5.4 Bounds on the Optimal Code Length](#bounds-on-the-optimal-code-length) (112)
- [5.5 Kraft Inequality for Uniquely Decodable Codes](#kraft-inequality-for-uniquely-decodable-codes) (115)
- [5.6 Huffman Codes](#huffman-codes) (118)
- [5.7 Some Comments on Huffman Codes](#some-comments-on-huffman-codes) (120)
- [5.8 Optimality of Huffman Codes](#optimality-of-huffman-codes) (123)
- [5.9 Shannon–Fano–Elias Coding](#shannon-fano-elias-coding) (127)
- [5.10 Competitive Optimality of the Shannon Code](#competitive-optimality-of-the-shannon-code) (130)
- [5.11 Generation of Discrete Distributions from Fair Coins](#generation-of-discrete-distributions-from-fair-coins) (134)
- [Summary](#summary-5) (141)
- [Problems](#problems-5) (142)
- [Historical Notes](#historical-notes-5) (157)

## [6. Gambling and Data Compression](#gambling-and-data-compression) (159)
- [6.1 The Horse Race](#the-horse-race) (159)
- [6.2 Gambling and Side Information](#gambling-and-side-information) (164)
- [6.3 Dependent Horse Races and Entropy Rate](#dependent-horse-races-and-entropy-rate) (166)
- [6.4 The Entropy of English](#the-entropy-of-english) (168)
- [6.5 Data Compression and Gambling](#data-compression-and-gambling) (171)
- [6.6 Gambling Estimate of the Entropy of English](#gambling-estimate-of-the-entropy-of-english) (173)
- [Summary](#summary-6) (175)
- [Problems](#problems-6) (176)
- [Historical Notes](#historical-notes-6) (182)

## [7. Channel Capacity](#channel-capacity) (183)
- [7.1 Examples of Channel Capacity](#examples-of-channel-capacity) (184)
  - [7.1.1 Noiseless Binary Channel](#noiseless-binary-channel) (184)
  - [7.1.2 Noisy Channel with Nonoverlapping Outputs](#noisy-channel-with-nonoverlapping-outputs) (185)
  - [7.1.3 Noisy Typewriter](#noisy-typewriter) (186)
  - [7.1.4 Binary Symmetric Channel](#binary-symmetric-channel) (187)
  - [7.1.5 Binary Erasure Channel](#binary-erasure-channel) (188)
- [7.2 Symmetric Channels](#symmetric-channels) (189)
- [7.3 Properties of Channel Capacity](#properties-of-channel-capacity) (191)
- [7.4 Preview of the Channel Coding Theorem](#preview-of-the-channel-coding-theorem) (191)
- [7.5 Definitions](#definitions) (192)
- [7.6 Jointly Typical Sequences](#jointly-typical-sequences) (195)
- [7.7 Channel Coding Theorem](#channel-coding-theorem) (199)
- [7.8 Zero-Error Codes](#zero-error-codes) (205)
- [7.9 Fano’s Inequality and the Converse to the Coding Theorem](#fanos-inequality-and-the-converse-to-the-coding-theorem) (206)
- [7.10 Equality in the Converse to the Channel Coding Theorem](#equality-in-the-converse-to-the-channel-coding-theorem) (208)
- [7.11 Hamming Codes](#hamming-codes) (210)
- [7.12 Feedback Capacity](#feedback-capacity) (216)
- [7.13 Source–Channel Separation Theorem](#source-channel-separation-theorem) (218)
- [Summary](#summary-7) (222)
- [Problems](#problems-7) (223)
- [Historical Notes](#historical-notes-7) (240)

## [8. Differential Entropy](#differential-entropy) (243)
- [8.1 Definitions](#definitions-8) (243)
- [8.2 AEP for Continuous Random Variables](#aep-for-continuous-random-variables) (245)
- [8.3 Relation of Differential Entropy to Discrete Entropy](#relation-of-differential-entropy-to-discrete-entropy) (247)
- [8.4 Joint and Conditional Differential Entropy](#joint-and-conditional-differential-entropy) (249)
- [8.5 Relative Entropy and Mutual Information](#relative-entropy-and-mutual-information-8) (250)
- [8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information](#properties-of-differential-entropy-relative-entropy-and-mutual-information) (252)
- [Summary](#summary-8) (256)
- [Problems](#problems-8) (256)
- [Historical Notes](#historical-notes-8) (259)

## [9. Gaussian Channel](#gaussian-channel) (261)
- [9.1 Gaussian Channel: Definitions](#gaussian-channel-definitions) (263)
- [9.2 Converse to the Coding Theorem for Gaussian Channels](#converse-to-the-coding-theorem-for-gaussian-channels) (268)
- [9.3 Bandlimited Channels](#bandlimited-channels) (270)
- [9.4 Parallel Gaussian Channels](#parallel-gaussian-channels) (274)
- [9.5 Channels with Colored Gaussian Noise](#channels-with-colored-gaussian-noise) (277)
- [9.6 Gaussian Channels with Feedback](#gaussian-channels-with-feedback) (280)
- [Summary](#summary-9) (289)
- [Problems](#problems-9) (290)
- [Historical Notes](#historical-notes-9) (299)

## [10. Rate Distortion Theory](#rate-distortion-theory) (301)
- [10.1 Quantization](#quantization) (301)
- [10.2 Definitions](#definitions-10) (303)
- [10.3 Calculation of the Rate Distortion Function](#calculation-of-the-rate-distortion-function) (307)
  - [10.3.1 Binary Source](#binary-source) (307)
  - [10.3.2 Gaussian Source](#gaussian-source) (310)
  - [10.3.3 Simultaneous Description of Independent Gaussian Random Variables](#simultaneous-description-of-independent-gaussian-random-variables) (312)
- [10.4 Converse to the Rate Distortion Theorem](#converse-to-the-rate-distortion-theorem) (315)
- [10.5 Achievability of the Rate Distortion Function](#achievability-of-the-rate-distortion-function) (318)
- [10.6 Strongly Typical Sequences and Rate Distortion](#strongly-typical-sequences-and-rate-distortion) (325)
- [10.7 Characterization of the Rate Distortion Function](#characterization-of-the-rate-distortion-function) (329)
- [10.8 Computation of Channel Capacity and the Rate Distortion Function](#computation-of-channel-capacity-and-the-rate-distortion-function) (332)
- [Summary](#summary-10) (335)
- [Problems](#problems-10) (336)
- [Historical Notes](#historical-notes-10) (345)

## [11. Information Theory and Statistics](#information-theory-and-statistics) (347)
- [11.1 Method of Types](#method-of-types) (347)
- [11.2 Law of Large Numbers](#law-of-large-numbers) (355)
- [11.3 Universal Source Coding](#universal-source-coding) (357)
- [11.4 Large Deviation Theory](#large-deviation-theory) (360)
- [11.5 Examples of Sanov’s Theorem](#examples-of-sanovs-theorem) (364)
- [11.6 Conditional Limit Theorem](#conditional-limit-theorem) (366)
- [11.7 Hypothesis Testing](#hypothesis-testing) (375)
- [11.8 Chernoff–Stein Lemma](#chernoff-stein-lemma) (380)
- [11.9 Chernoff Information](#chernoff-information) (384)
- [11.10 Fisher Information and the Cramér–Rao Inequality](#fisher-information-and-the-cramer-rao-inequality) (392)
- [Summary](#summary-11) (397)
- [Problems](#problems-11) (399)
- [Historical Notes](#historical-notes-11) (408)

## [12. Maximum Entropy](#maximum-entropy) (409)
- [12.1 Maximum Entropy Distributions](#maximum-entropy-distributions) (409)
- [12.2 Examples](#examples) (411)
- [12.3 Anomalous Maximum Entropy Problem](#anomalous-maximum-entropy-problem) (413)
- [12.4 Spectrum Estimation](#spectrum-estimation) (415)
- [12.5 Entropy Rates of a Gaussian Process](#entropy-rates-of-a-gaussian-process) (416)
- [12.6 Burg’s Maximum Entropy Theorem](#burgs-maximum-entropy-theorem) (417)
- [Summary](#summary-12) (420)
- [Problems](#problems-12) (421)
- [Historical Notes](#historical-notes-12) (425)

## [13. Universal Source Coding](#universal-source-coding-13) (427)
- [13.1 Universal Codes and Channel Capacity](#universal-codes-and-channel-capacity) (428)
- [13.2 Universal Coding for Binary Sequences](#universal-coding-for-binary-sequences) (433)
- [13.3 Arithmetic Coding](#arithmetic-coding) (436)
- [13.4 Lempel–Ziv Coding](#lempel-ziv-coding) (440)
  - [13.4.1 Sliding Window Lempel–Ziv Algorithm](#sliding-window-lempel-ziv-algorithm) (441)
  - [13.4.2 Tree-Structured Lempel–Ziv Algorithms](#tree-structured-lempel-ziv-algorithms) (442)
- [13.5 Optimality of Lempel–Ziv Algorithms](#optimality-of-lempel-ziv-algorithms) (443)
  - [13.5.1 Sliding Window Lempel–Ziv Algorithms](#sliding-window-lempel-ziv-algorithms) (443)
  - [13.5.2 Optimality of Tree-Structured Lempel–Ziv Compression](#optimality-of-tree-structured-lempel-ziv-compression) (448)
- [Summary](#summary-13) (456)
- [Problems](#problems-13) (457)
- [Historical Notes](#historical-notes-13) (461)

## [14. Kolmogorov Complexity](#kolmogorov-complexity) (463)
- [14.1 Models of Computation](#models-of-computation) (464)
- [14.2 Kolmogorov Complexity: Definitions and Examples](#kolmogorov-complexity-definitions-and-examples) (466)
- [14.3 Kolmogorov Complexity and Entropy](#kolmogorov-complexity-and-entropy) (473)
- [14.4 Kolmogorov Complexity of Integers](#kolmogorov-complexity-of-integers) (475)
- [14.5 Algorithmically Random and Incompressible Sequences](#algorithmically-random-and-incompressible-sequences) (476)
- [14.6 Universal Probability](#universal-probability) (480)
- [14.7 Kolmogorov Complexity](#kolmogorov-complexity-14) (482)
- [14.8 Universal Gambling](#universal-gambling) (487)
- [14.9 Occam’s Razor](#occams-razor) (488)
- [14.10 Kolmogorov Complexity and Universal Probability](#kolmogorov-complexity-and-universal-probability) (490)
- [14.11 Kolmogorov Sufficient Statistic](#kolmogorov-sufficient-statistic) (496)
- [14.12 Minimum Description Length Principle](#minimum-description-length-principle) (500)
- [Summary](#summary-14) (501)
- [Problems](#problems-14) (503)
- [Historical Notes](#historical-notes-14) (507)

## [15. Network Information Theory](#network-information-theory) (509)
- [15.1 Gaussian Multiple-User Channels](#gaussian-multiple-user-channels) (513)
  - [15.1.1 Single-User Gaussian Channel](#single-user-gaussian-channel) (513)
  - [15.1.2 Gaussian Multiple-Access Channel with m Users](#gaussian-multiple-access-channel-with-m-users) (514)
  - [15.1.3 Gaussian Broadcast Channel](#gaussian-broadcast-channel) (515)
  - [15.1.4 Gaussian Relay Channel](#gaussian-relay-channel) (516)
  - [15.1.5 Gaussian Interference Channel](#gaussian-interference-channel) (518)
  - [15.1.6 Gaussian Two-Way Channel](#gaussian-two-way-channel) (519)
- [15.2 Jointly Typical Sequences](#jointly-typical-sequences-15) (520)
- [15.3 Multiple-Access Channel](#multiple-access-channel) (524)
  - [15.3.1 Achievability of the Capacity Region for the Multiple-Access Channel](#achievability-of-the-capacity-region-for-the-multiple-access-channel) (530)
  - [15.3.2 Comments on the Capacity Region for the Multiple-Access Channel](#comments-on-the-capacity-region-for-the-multiple-access-channel) (532)
  - [15.3.3 Convexity of the Capacity Region of the Multiple-Access Channel](#convexity-of-the-capacity-region-of-the-multiple-access-channel) (534)
  - [15.3.4 Converse for the Multiple-Access Channel](#converse-for-the-multiple-access-channel) (538)
  - [15.3.5 m-User Multiple-Access Channels](#m-user-multiple-access-channels) (543)
  - [15.3.6 Gaussian Multiple-Access Channels](#gaussian-multiple-access-channels) (544)
- [15.4 Encoding of Correlated Sources](#encoding-of-correlated-sources) (549)
  - [15.4.1 Achievability of the Slepian–Wolf Theorem](#achievability-of-the-slepian-wolf-theorem) (551)
  - [15.4.2 Converse for the Slepian–Wolf Theorem](#converse-for-the-slepian-wolf-theorem) (555)
  - [15.4.3 Slepian–Wolf Theorem for Many Sources](#slepian-wolf-theorem-for-many-sources) (556)
  - [15.4.4 Interpretation of Slepian–Wolf Coding](#interpretation-of-slepian-wolf-coding) (557)
- [15.5 Duality Between Slepian–Wolf Encoding and Multiple-Access Channels](#duality-between-slepian-wolf-encoding-and-multiple-access-channels) (558)
- [15.6 Broadcast Channel](#broadcast-channel) (560)
  - [15.6.1 Definitions for a Broadcast Channel](#definitions-for-a-broadcast-channel) (563)
  - [15.6.2 Degraded Broadcast Channels](#degraded-broadcast-channels) (564)
  - [15.6.3 Capacity Region for the Degraded Broadcast Channel](#capacity-region-for-the-degraded-broadcast-channel) (565)
- [15.7 Relay Channel](#relay-channel) (571)
- [15.8 Source Coding with Side Information](#source-coding-with-side-information) (575)
- [15.9 Rate Distortion with Side Information](#rate-distortion-with-side-information) (580)
- [15.10 General Multiterminal Networks](#general-multiterminal-networks) (587)
- [Summary](#summary-15) (594)
- [Problems](#problems-15) (596)
- [Historical Notes](#historical-notes-15) (609)

## [16. Information Theory and Portfolio Theory](#information-theory-and-portfolio-theory) (613)
- [16.1 The Stock Market: Some Definitions](#the-stock-market-some-definitions) (613)
- [16.2 Kuhn–Tucker Characterization of the Log-Optimal Portfolio](#kuhn-tucker-characterization-of-the-log-optimal-portfolio) (617)
- [16.3 Asymptotic Optimality of the Log-Optimal Portfolio](#asymptotic-optimality-of-the-log-optimal-portfolio) (619)
- [16.4 Side Information and the Growth Rate](#side-information-and-the-growth-rate) (621)
- [16.5 Investment in Stationary Markets](#investment-in-stationary-markets) (623)
- [16.6 Competitive Optimality of the Log-Optimal Portfolio](#competitive-optimality-of-the-log-optimal-portfolio) (627)
- [16.7 Universal Portfolios](#universal-portfolios) (629)
  - [16.7.1 Finite-Horizon Universal Portfolios](#finite-horizon-universal-portfolios) (631)
  - [16.7.2 Horizon-Free Universal Portfolios](#horizon-free-universal-portfolios) (638)
- [16.8 Shannon–McMillan–Breiman Theorem (General AEP)](#shannon-mcmillan-breiman-theorem-general-aep) (644)
- [Summary](#summary-16) (650)
- [Problems](#problems-16) (652)
- [Historical Notes](#historical-notes-16) (655)

## [17. Inequalities in Information Theory](#inequalities-in-information-theory) (657)
- [17.1 Basic Inequalities of Information Theory](#basic-inequalities-of-information-theory) (657)
- [17.2 Differential Entropy](#differential-entropy-17) (660)
- [17.3 Bounds on Entropy and Relative Entropy](#bounds-on-entropy-and-relative-entropy) (663)
- [17.4 Inequalities for Types](#inequalities-for-types) (665)
- [17.5 Combinatorial Bounds on Entropy](#combinatorial-bounds-on-entropy) (666)
- [17.6 Entropy Rates of Subsets](#entropy-rates-of-subsets) (667)
- [17.7 Entropy and Fisher Information](#entropy-and-fisher-information) (671)
- [17.8 Entropy Power Inequality and Brunn–Minkowski Inequality](#entropy-power-inequality-and-brunn-minkowski-inequality) (674)
- [17.9 Inequalities for Determinants](#inequalities-for-determinants) (679)
- [17.10 Inequalities for Ratios of Determinants](#inequalities-for-ratios-of-determinants) (683)
- [Summary](#summary-17) (686)
- [Problems](#problems-17) (686)
- [Historical Notes](#historical-notes-17) (687)

- [Bibliography](#bibliography) (689)
- [List of Symbols](#list-of-symbols) (723)
- [Index](#index) (727)